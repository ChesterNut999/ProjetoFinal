{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_PySpark",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarianaDuartee/ProjetoFinal/blob/main/2_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wox7CNOGJGrF"
      },
      "source": [
        "### INSTALANDO DEPENDECIAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG4awT5LJEmS"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install gcsfs\n",
        "!pip install pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVW92KejMHoY"
      },
      "source": [
        "### IMPORTANDO BIBLIOTECAS, ABRINDO SPARKSESSION E CONFIGURANDO CHAVE DE SERVIÇO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA3IQkKCL2sI"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SQLContext\n",
        "from google.cloud import storage\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.types import DateType\n",
        "from pyspark.sql import Window\n",
        "\n",
        "import pyspark\n",
        "import pyspark.sql.functions as F\n",
        "import os\n",
        "import gcsfs\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO9olZPDMpxb"
      },
      "source": [
        "spark = SparkSession.builder\\\n",
        ".master('local')\\\n",
        ".appName('Projeto_Final')\\\n",
        ".config('spark.ui.enable', 'true')\\\n",
        ".config('spark.ui.port', '4050')\\\n",
        ".getOrCreate()\n",
        "\n",
        "spark\n",
        "\n",
        "serviceaccount = '/content/soulcode-projeto-final-4b88bea6e07a.json'\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = serviceaccount\n",
        "\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7EKwiUaOZ7N"
      },
      "source": [
        "### IMPORTANDO, EXTRAINDO INFORMAÇÕES E ANALISANDO DATAFRAMES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjEFDl68OhtS"
      },
      "source": [
        "'''https://acervolima.com/como-converter-pandas-para-pyspark-dataframe/'''\n",
        "\n",
        "# ARQUIVO 1\n",
        "file_path_1 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_despesas_normalizado.csv'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_1):\n",
        "    data = pd.read_csv(file_path_1, sep=',', encoding='UTF-8', header=0)\n",
        "\n",
        "df_1 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "#ARQUIVO 2\n",
        "file_path_2 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_ocorrencias_normalizado.json'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_2):\n",
        "    data = pd.read_json(file_path_2, encoding='UTF-8')\n",
        "\n",
        "df_2 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "#ARQUIVO 3\n",
        "file_path_3 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_vitimas_normalizado.json'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_3):\n",
        "    data = pd.read_json(file_path_3, encoding='UTF-8')\n",
        "\n",
        "df_3 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "#ARQUIVO 4\n",
        "file_path_4 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_ocorrencias_normalizado.json'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_4):\n",
        "    data = pd.read_json(file_path_4, encoding='UTF-8')\n",
        "\n",
        "df_4 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "#ARQUIVO 5\n",
        "file_path_5 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_ocorrencia_vitimas_porAnoEstado.csv'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_5):\n",
        "    data = pd.read_csv(file_path_5, sep=',', encoding='UTF-8')\n",
        "\n",
        "df_5 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# ARQUIVO 6\n",
        "file_path_6 = 'gs://data_lake_ingest_data/1_input/Tabela_frequencia_escolar.xlsx'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_6):\n",
        "    data = pd.read_excel(file_path_6, header=0)\n",
        "\n",
        "df_6 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# ARQUIVO 7\n",
        "file_path_7 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_taxa_analfabetismo_normalizado.csv'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_7):\n",
        "    data = pd.read_csv(file_path_7, header=0)\n",
        "\n",
        "df_7 = spark.createDataFrame(data)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "df_1.printSchema()\n",
        "df_1.show(5, truncate=False)\n",
        "df_1.dtypes\n",
        "\n",
        "df_2.printSchema()\n",
        "df_2.show(5, truncate=False)\n",
        "df_2.dtypes\n",
        "\n",
        "df_3.printSchema()\n",
        "df_3.show(5, truncate=False)\n",
        "df_3.dtypes\n",
        "\n",
        "df_4.printSchema()\n",
        "df_4.show(5, truncate=False)\n",
        "df_4.dtypes\n",
        "\n",
        "df_5.printSchema()\n",
        "df_5.show(5, truncate=False)\n",
        "df_5.dtypes\n",
        "\n",
        "df_6.printSchema()\n",
        "df_6.show(5, truncate=False)\n",
        "df_6.dtypes\n",
        "\n",
        "df_7.printSchema()\n",
        "df_7.show(5, truncate=False)\n",
        "df_7.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsZ927gDOKDx"
      },
      "source": [
        "### NORMALIZANDO DATAFRAMES\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAPe60SHVbP6"
      },
      "source": [
        "# DATAFRAME 1\n",
        "\n",
        "df_1 = df_1.drop('Estimativa_2021', 'Previsao2021Media')\n",
        "df_1 = df_1.drop('Estimativa_2021', 'Previsao2021Media')\n",
        "df_1 = df_1.withColumnRenamed('Media Anual', 'Estimativa_Despesa_2021')\n",
        "df_1 = df_1.withColumnRenamed('Previsao2021|Media', 'Previsao2021_Media')\n",
        "\n",
        "df_1.withColumn(\"Despesas2017\", F.round(df_1.Despesas2016.cast(FloatType()), 3))\n",
        "df_1.withColumn(\"Despesas2017\", F.round(df_1.Despesas2017.cast(FloatType()), 3))\n",
        "df_1.withColumn(\"Despesas2018\", F.round(df_1.Despesas2018.cast(FloatType()), 3))\n",
        "df_1.withColumn(\"Despesas2019\", F.round(df_1.Despesas2019.cast(FloatType()), 3))\n",
        "df_1.withColumn(\"Despesas2020\", F.round(df_1.Despesas2020.cast(FloatType()), 3))\n",
        "df_1.withColumn(\"Previsao2021_Media\", F.round(df_1.Previsao2021_Media.cast(FloatType()), 3))\n",
        "\n",
        "df_1.show()\n",
        "df_1.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfQ55azXerU3"
      },
      "source": [
        "# DATAFRAME 6\n",
        "\n",
        "# DROPANDO COLUNAS\n",
        "df_6 = df_6.drop('15 a 17 anos - 2019', \n",
        "                 '15 a 17 anos - 2018',\n",
        "                 '15 a 17 anos - 2017',\n",
        "                 '15 a 17 anos - 2016',\n",
        "                 '18 a 24 anos - 2019',\n",
        "                 '18 a 24 anos - 2018',\n",
        "                 '18 a 24 anos - 2017',\n",
        "                 '18 a 24 anos - 2016',\n",
        "                 '25 anos ou mais - 2019',\n",
        "                 '25 anos ou mais - 2018',\n",
        "                 '25 anos ou mais - 2017',\n",
        "                 '25 anos ou mais - 2016')\n",
        "\n",
        "# RENOMEANDO COLUNAS FAIXA DE IDADE\n",
        "df_6 = df_6.withColumnRenamed('Grandes Regiões, Unidades da Federação e Municípios das Capitais', 'UF')\n",
        "df_6 = df_6.withColumnRenamed('Media 15 a 17 anos', 'Media_Freq_15_a_17_anos')\n",
        "df_6 = df_6.withColumnRenamed('média 18 a 24 anos', 'Media_Freq_18_a_24_anos')\n",
        "df_6 = df_6.withColumnRenamed('média 25 anos ou mais', 'Media_Freq_25_anos_ou_mais')\n",
        "\n",
        "# FILTRO PARA REMOVER REGIÕES E CAPITAIS\n",
        "df_6 = df_6.filter((df_6.UF != \"Brasil\") & \n",
        "            (df_6.UF != \"Norte\") & \n",
        "            (df_6.UF != \"Nordeste\") &\n",
        "            (df_6.UF != \"Sudeste\") & \n",
        "            (df_6.UF != \"Sul\") &\n",
        "            (df_6.UF != \"Centro-Oeste\") &\n",
        "            (df_6.UF != \"Porto Velho\") &\n",
        "            (df_6.UF != \"Rio Branco\") &\n",
        "            (df_6.UF != \"Manaus\") &\n",
        "            (df_6.UF != \"Boa Vista\") &\n",
        "            (df_6.UF != \"Macapá\") &\n",
        "            (df_6.UF != \"Palmas\") &\n",
        "            (df_6.UF != \"São Luiz\") &\n",
        "            (df_6.UF != \"Teresina\") &\n",
        "            (df_6.UF != \"Fortaleza\") &\n",
        "            (df_6.UF != \"Natal\") &\n",
        "            (df_6.UF != \"joão Pessoa\") &\n",
        "            (df_6.UF != \"Recife\") &\n",
        "            (df_6.UF != \"Maceió\") &\n",
        "            (df_6.UF != \"Aracaju\") &\n",
        "            (df_6.UF != \"Salvador\") &\n",
        "            (df_6.UF != \"Belo Horizonte\") &\n",
        "            (df_6.UF != \"Vitória\") &\n",
        "            (df_6.UF != \"Curitiba\") &\n",
        "            (df_6.UF != \"Florianópolis\") &\n",
        "            (df_6.UF != \"Porto Alegre\") &\n",
        "            (df_6.UF != \"Campo Grande\") &\n",
        "            (df_6.UF != \"Cuiabá\") &\n",
        "            (df_6.UF != \"Goiânia\") &\n",
        "            (df_6.UF != \"Brasilia\"))\\\n",
        "            .orderBy('UF', ascending=True)\n",
        "\n",
        "# # INCREMENTANDO TAXA DE EVASÃO\n",
        "# df_6 = df_6.withColumn('Evasao_11_a_14_anos', (df_5.Freq_11_a_14_anos - 100)) \n",
        "# df_6 = df_6.withColumn('Evasao_15_a_17_anos', (df_5.Freq_11_a_14_anos - 100))\n",
        "# df_6 = df_6.withColumn('Evasao_18_a_24_anos', (df_5.Freq_11_a_14_anos - 100))\n",
        "# df_6 = df_6.withColumn('Evasao_25_anos_ou_mais', (df_5.Freq_11_a_14_anos - 100))\n",
        "\n",
        "# CONVERTENDO VALORES EM PORCENTAGEM\n",
        "'''https://stackoverflow.com/questions/60673912/how-to-convert-number-into-percentage'''\n",
        "\n",
        "df_6 = df_6\\\n",
        ".withColumn(\"Media_Freq_15_a_17_anos\", F.concat((F.col(\"Media_Freq_15_a_17_anos\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        ".withColumn(\"Media_Freq_18_a_24_anos\", F.concat((F.col(\"Media_Freq_18_a_24_anos\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        ".withColumn(\"Media_Freq_25_anos_ou_mais\", F.concat((F.col(\"Media_Freq_25_anos_ou_mais\") * 1).cast(\"int\"), F.lit(' %')))\n",
        "# .withColumn(\"Media_Freq_Escolar\", F.concat((F.col(\"Media_Freq_Escolar\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        "# .withColumn(\"Evasao_11_a_14_anos\", F.concat((F.col(\"Evasao_11_a_14_anos\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        "# .withColumn(\"Evasao_15_a_17_anos\", F.concat((F.col(\"Evasao_15_a_17_anos\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        "# .withColumn(\"Evasao_18_a_24_anos\", F.concat((F.col(\"Evasao_18_a_24_anos\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        "# .withColumn(\"Evasao_25_anos_ou_mais\", F.concat((F.col(\"Evasao_25_anos_ou_mais\") * 1).cast(\"int\"), F.lit(' %')))\n",
        "\n",
        "# AJUSTANDO EXIBIÇÃO DO DATAFRAME\n",
        "df_6 = df_6.select('UF', \n",
        "                   'Media_Freq_15_a_17_anos', \n",
        "                #    'Evasao_11_a_14_anos',\n",
        "                   'Media_Freq_18_a_24_anos',\n",
        "                #    'Evasao_15_a_17_anos',\n",
        "                   'Media_Freq_25_anos_ou_mais',\n",
        "                #    'Evasao_18_a_24_anos',\n",
        "                #    'Freq_25_anos_ou_mais',\n",
        "                #    'Evasao_25_anos_ou_mais',\n",
        "                #    'Media_Freq_Escolar'\n",
        "               )\n",
        "\n",
        "df_6.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU5Gz3ShKsoA"
      },
      "source": [
        "# TRANSFORMARDO DATAFRAME PYSPAK EM PANDAS\n",
        "'''https://sparkbyexamples.com/pyspark/convert-pyspark-dataframe-to-pandas/'''\n",
        "\n",
        "df_6 = df_6.toPandas()\n",
        "df_6.to_csv('temp_pyspark_tabela_frequencia_escolar_normalizado.csv')\n",
        "\n",
        "BUCKET_NAME= \"data_lake_ingest_data\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "\n",
        "fileout = \"2_temp/temp_pyspark_tabela_frequencia_escolar_normalizado.csv\"\n",
        "destination_blob = bucket.blob(fileout)\n",
        "destination_blob.upload_from_filename('/content/temp_pyspark_tabela_frequencia_escolar_normalizado.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Flsf5qzGHp"
      },
      "source": [
        "# DATAFRAME 7\n",
        "df_7 = df_7.drop('Grupo_idade')\n",
        "df_7 = df_7.drop('Unnamed: 0')\n",
        "\n",
        "df_7 = df_7\\\n",
        ".withColumn(\"TaxaAnalfabetismo2016\", F.concat((F.col(\"TaxaAnalfabetismo2016\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        ".withColumn(\"TaxaAnalfabetismo2017\", F.concat((F.col(\"TaxaAnalfabetismo2017\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        ".withColumn(\"TaxaAnalfabetismo2018\", F.concat((F.col(\"TaxaAnalfabetismo2018\") * 1).cast(\"int\"), F.lit(' %')))\\\n",
        ".withColumn(\"TaxaAnalfabetismo2019\", F.concat((F.col(\"TaxaAnalfabetismo2019\") * 1).cast(\"int\"), F.lit(' %')))\n",
        "\n",
        "df_7.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-SnL3GEesv8"
      },
      "source": [
        "### CONSULTAS / AGREGANDO DADOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2bgQFezOJDR"
      },
      "source": [
        "# VISUALIZAR DATAFRAMES TRATADOS\n",
        "\n",
        "df_1.show(5, truncate=False)\n",
        "df_2.show(5, truncate=False)\n",
        "df_3.show(5, truncate=False)\n",
        "df_4.show(5, truncate=False)\n",
        "df_5.show(5, truncate=False)\n",
        "df_6.show(5, truncate=False)\n",
        "df_7.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHFqf-0VAulA"
      },
      "source": [
        "JOINS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsswAo5erQl-"
      },
      "source": [
        "# AGREGAÇÃO 1\n",
        "\n",
        "#AGRUPANDO VALORES DOS DATAFRAMES\n",
        "df_ins2 = df_2.groupBy('UF').sum('Ocorrencias').orderBy('UF')\n",
        "df_ins2.show(50, truncate=False)\n",
        "\n",
        "df_ins3 = df_3.groupBy('UF').sum('Vitimas').orderBy('UF')\n",
        "df_ins3.show(50, truncate=False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# JOIN 1\n",
        "# AGREGA OS VALORES TOTAIS DE OCORRENCIAS E VITIMAS\n",
        "\n",
        "df_join_1 = df_ins2.join(df_ins3, on=['UF'], how='inner')\\\n",
        ".select('UF', 'sum(Ocorrencias)', 'sum(Vitimas)')\\\n",
        ".orderBy(F.asc('UF'))\n",
        "\n",
        "df_join_1.show(50, False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# JOIN 2\n",
        "# AGREGA OS VALORES TOTAIS DE OCORRENCIAS, VITIMAS, MEDIA DE FREQ ESCOLAR\n",
        "\n",
        "df_join_1 = df_join_1.join(df_6, on=['UF'], how='inner')\\\n",
        ".select('UF', 'sum(Ocorrencias)', 'sum(Vitimas)', 'Media_Freq_25_anos_ou_mais')\\\n",
        ".orderBy('UF', ascending=True)\n",
        "\n",
        "df_join_1.show(50, False)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# JOIN 3\n",
        "# AGREGA OS VALORES TOTAIS DE DESPESAS GOVERNAMENTAIS, OCORRENCIAS, VITIMAS, MEDIA DE FREQ. ESCOLAR\n",
        "\n",
        "df_join_1 = df_join_1.join(df_1, on=['UF'], how='inner')\\\n",
        ".select('UF', 'Despesas2020', 'sum(Ocorrencias)', 'sum(Vitimas)', 'Media_Freq_25_anos_ou_mais')\\\n",
        ".orderBy(F.asc('UF'))\n",
        "\n",
        "df_join_1\\\n",
        ".orderBy(\"Despesas2020\", ascending=False)\\\n",
        ".show(50, False)\n",
        "\n",
        "df_join_1.dtypes\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_join_1 = df_join_1.toPandas()\n",
        "df_join_1.to_csv('temp_pyspark_agg_1_General_Data.csv')\n",
        "\n",
        "BUCKET_NAME= \"data_lake_ingest_data\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "\n",
        "fileout = \"2_temp/temp_pyspark_agg_1_General_Data.csv\"\n",
        "destination_blob = bucket.blob(fileout)\n",
        "destination_blob.upload_from_filename('/content/temp_pyspark_agg_1_General_Data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TofPZBof0Npu"
      },
      "source": [
        "# AGREGAÇÃO 2\n",
        "\n",
        "# JOIN 1\n",
        "# AGREGA OS VALORES TOTAIS DE OCORRENCIAS E VITIMAS POR ESTADO \n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_join_1 = df_6.join(df_7, on=['UF'], how='inner')\\\n",
        ".select('UF', 'Media_Freq_15_a_17_anos', 'Media_Freq_18_a_24_anos', 'Media_Freq_25_anos_ou_mais', \n",
        "        'TaxaAnalfabetismo2016', 'TaxaAnalfabetismo2017', 'TaxaAnalfabetismo2018', 'TaxaAnalfabetismo2019')\\\n",
        ".orderBy(F.asc('UF'))\\\n",
        "\n",
        "df_join_1.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_join_1 = df_join_1.toPandas()\n",
        "df_join_1.to_csv('temp_pyspark_agg_2_Freq_Escolar_Taxa_Analfa.csv')\n",
        "\n",
        "BUCKET_NAME= \"data_lake_ingest_data\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "\n",
        "fileout = \"2_temp/temp_pyspark_agg_2_Freq_Escolar_Taxa_Analfa.csv\"\n",
        "destination_blob = bucket.blob(fileout)\n",
        "destination_blob.upload_from_filename('/content/temp_pyspark_agg_2_Freq_Escolar_Taxa_Analfa.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWfPjmVJkZTR"
      },
      "source": [
        "# AGREGAÇÃO 3\n",
        "\n",
        "# AGRUPANDO VALORES DOS DATAFRAMES\n",
        "df_agg_1 = df_2.filter(df_2.Ano == 2020)\\\n",
        ".groupBy('UF').sum('Ocorrencias').alias('Ocorrencias_2020').orderBy('UF')\n",
        "\n",
        "df_agg_1.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_agg_2 = df_3.filter(df_3.Ano == 2020)\\\n",
        ".groupBy('UF').sum('Vitimas').alias('Vitimas_2020').orderBy('UF')\n",
        "\n",
        "df_agg_2.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# AGREGANDO OS VALORES PELO ANO DE 2020\n",
        "\n",
        "df_join_1 = df_agg_1.join(df_agg_2, on=['UF'], how='inner')\\\n",
        ".select('UF', 'sum(Ocorrencias)', 'sum(Vitimas)')\\\n",
        ".orderBy('UF', ascending=True)\n",
        "\n",
        "df_join_1.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_join_2 = df_1.select('UF', 'Despesas2020')\n",
        "\n",
        "df_join_1 = df_join_1.join(df_join_2, on=['UF'], how='inner')\\\n",
        ".select('UF', \n",
        "        'sum(Ocorrencias)', \n",
        "        'sum(Vitimas)', \n",
        "        'Despesas2020')\n",
        "\n",
        "df_join_1 = df_join_1\\\n",
        ".withColumnRenamed('sum(Ocorrencias)', 'Ocorrencias_2020')\\\n",
        ".withColumnRenamed('sum(Vitimas)', 'Vitimas_2020')\\\n",
        ".withColumnRenamed('Despesas2020', 'Despesas_Gov_2020')\\\n",
        "\n",
        "df_join_1.show()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "df_join_1 = df_join_1.toPandas()\n",
        "df_join_1.to_csv('temp_pyspark_agg_3_general_data_2020.csv')\n",
        "\n",
        "BUCKET_NAME= \"data_lake_ingest_data\"\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "\n",
        "fileout = \"2_temp/temp_pyspark_agg_3_general_data_2020.csv\"\n",
        "destination_blob = bucket.blob(fileout)\n",
        "destination_blob.upload_from_filename('/content/temp_pyspark_agg_3_general_data_2020.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8894hv-Gpfam"
      },
      "source": [
        "WINDOW FUNCTIONS RANKING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpk-tMEv6sP7"
      },
      "source": [
        "# ANALISANDO AS OCORRENCIAS DO ESTADO SP EM 2019\n",
        "\n",
        "df_2_consulta = df_2.groupby('UF', 'TipoCrime', 'Ano')\\\n",
        ".sum('Ocorrencias')\\\n",
        ".filter(\n",
        "    (df_2.UF == \"São Paulo\") & \n",
        "    (df_2.Ano == 2019))\\\n",
        ".orderBy('TipoCrime')\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# RANKING DAS OCORRENCIAS\n",
        "w0 = Window.partitionBy(F.col('UF')).orderBy(F.desc('sum(Ocorrencias)'))\n",
        "\n",
        "df_2_consulta.withColumn('Ranking', F.row_number().over(w0))\\\n",
        ".show(truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYSQUkOqpkkm"
      },
      "source": [
        "STRUCT TYPE | UNION BY NAME"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ySur1R_FzeF"
      },
      "source": [
        "# CRIANDO DATAFRAME VAZIO\n",
        "\n",
        "esquema = (StructType([\n",
        "        StructField('UF', StringType(), True),\n",
        "        StructField('TipoCrime', StringType(), True),\n",
        "        StructField('Ano', IntegerType(), True),\n",
        "        StructField('Mes', StringType(), True),\n",
        "        StructField('SexoVitima', StringType(), True),\n",
        "        StructField('Vitimas', IntegerType(), True)\n",
        "    ])\n",
        ")\n",
        "\n",
        "df_3_consulta = spark.createDataFrame(data='', schema=esquema)\n",
        "df_3_consulta.printSchema()\n",
        "df_3_consulta.show()\n",
        "\n",
        "# UNINDO DF\n",
        "\n",
        "df_3_consulta.unionByName(df_3)\\\n",
        ".orderBy('Ano', ascending=False)\\\n",
        ".show(5, truncate=False)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# EXIBINDO AS QTD. DE VITIMAS DE CRIMES DO ESTADO RJ EM 2020 POR SEXO\n",
        "\n",
        "df_3_consulta = df_3.groupby('UF', 'TipoCrime', 'Ano', 'SexoVitima')\\\n",
        ".sum('Vitimas')\\\n",
        ".filter(\n",
        "    (df_3.UF == \"Rio de Janeiro\") &\n",
        "    (df_3.Ano == 2020) &\n",
        "    (df_3.SexoVitima == \"Feminino\"))\\\n",
        ".orderBy('TipoCrime')\n",
        "\n",
        "df_3_consulta.show(truncate=False)\n",
        "\n",
        "df_3_consulta = df_3.groupby('UF', 'TipoCrime', 'Ano', 'SexoVitima')\\\n",
        ".sum('Vitimas')\\\n",
        ".filter(\n",
        "    (df_3.UF == \"Rio de Janeiro\") &\n",
        "    (df_3.Ano == 2020) &\n",
        "    (df_3.SexoVitima == \"Masculino\"))\\\n",
        ".orderBy('TipoCrime')\n",
        "\n",
        "df_3_consulta.show(truncate=False)\n",
        "\n",
        "df_3_consulta = df_3.groupby('UF', 'TipoCrime', 'Ano', 'SexoVitima')\\\n",
        ".sum('Vitimas')\\\n",
        ".filter(\n",
        "    (df_3.UF == \"Rio de Janeiro\") &\n",
        "    (df_3.Ano == 2020) &\n",
        "    (df_3.SexoVitima == \"Sexo NI\"))\\\n",
        ".orderBy('TipoCrime')\n",
        "\n",
        "df_3_consulta.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZQVT_sss3Rf"
      },
      "source": [
        "HEADER | AGGREGATE | SUM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyui-5cpTgOT"
      },
      "source": [
        "# ALTERANDO CABEÇALHO\n",
        "\n",
        "file_path_1 = 'gs://data_lake_ingest_data/2_temp/temp_pandas_despesas_normalizado.csv'\n",
        "\n",
        "fs = gcsfs.GCSFileSystem(project='soulcode-projeto-final', token=serviceaccount)\n",
        "with fs.open(file_path_1):\n",
        "    data = pd.read_csv(file_path_1,\n",
        "                       sep=',', \n",
        "                       encoding='UTF-8')\n",
        "\n",
        "esquema=['UF', \n",
        "         'Desp_2016', \n",
        "         'Desp_2017', \n",
        "         'Desp_2018', \n",
        "         'Desp_2019', \n",
        "         'Desp_2020', \n",
        "         'Variacao_%', \n",
        "         'Previsao_2021_Media']\n",
        "\n",
        "df_1_consulta = spark.createDataFrame(data, schema=esquema)\n",
        "\n",
        "df_1_consulta.show(5, truncate=False)\n",
        "\n",
        "# --------------------------------------------\n",
        "\n",
        "# AGREGANDO O TOTAL DE DESPESAS GOVERNAMENTAIS DO PARANA REFERENTES AOS ULTIMOS 3 ANOS\n",
        "\n",
        "df_1_consulta.select('UF', 'Desp_2018', 'Desp_2019', 'Desp_2020')\\\n",
        ".filter((df_1_consulta.UF == 'Paraná'))\\\n",
        ".groupBy('UF', 'Desp_2018', 'Desp_2019', 'Desp_2020')\\\n",
        ".agg(F.sum(df_1_consulta.Desp_2018 + \n",
        "           df_1_consulta.Desp_2019 +\n",
        "           df_1_consulta.Desp_2020).alias('PR_Despesas_Totais_Milhoes'))\\\n",
        ".show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeID-8LSEBLn"
      },
      "source": [
        "FILTERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdnQjVNVDTjd"
      },
      "source": [
        "# FILTRANDO INTERVALOS\n",
        "\n",
        "# IGUAL A 90% E MAIOR QUE 20 %\n",
        "df_6.filter(\n",
        "    (df_6.Media_Freq_15_a_17_anos == '90 %') & \n",
        "    (df_6.Media_Freq_15_a_17_anos > '20 %'))\\\n",
        ".select('UF', 'Media_Freq_15_a_17_anos')\\\n",
        ".orderBy(F.desc('Media_Freq_15_a_17_anos'))\\\n",
        ".show(5, truncate=False)\n",
        "\n",
        "# IGUAL E MENOR A 40 % OU IGUAL E MAIOR QUE 20 %\n",
        "df_6.filter(\n",
        "    (df_6.Media_Freq_18_a_24_anos <= '40 %') |\n",
        "    (df_6.Media_Freq_18_a_24_anos >= '20 %'))\\\n",
        ".select('UF', 'Media_Freq_18_a_24_anos')\\\n",
        ".orderBy(F.desc('Media_Freq_18_a_24_anos'))\\\n",
        ".show(5, truncate=False)\n",
        "\n",
        "# IGUAL E MENOR QUE 20 % OU MAIOR QUE 8 %\n",
        "df_6.filter(\n",
        "    (df_6.Media_Freq_25_anos_ou_mais <= '20 %') |\n",
        "    (df_6.Media_Freq_25_anos_ou_mais > '8 %'))\\\n",
        ".select('UF', 'Media_Freq_25_anos_ou_mais')\\\n",
        ".orderBy(F.desc('Media_Freq_25_anos_ou_mais'))\\\n",
        ".show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}